# ============================================================================
# Experimental verification of Algorithm 1
# ============================================================================
#
# Purpose
# -------
# This Sage/Python script experimentally validates (a small-parameter version of)
# ‚ÄúAlgorithm 1‚Äù by:
#   1) precomputing Compatibility Table and swap_sequences via SJT,
#   2) generating a random PKP instance (A,b) over GF(q) with a planted solution V,
#   3) performing the algorithm‚Äôs reduction steps,
#   4) running the meet-in-the-middle style collision search,
#   5) counting (raw) collisions and ‚Äúfiltered‚Äù collisions, and checking whether
#      the planted solution is recovered.
#
# Notes
# -----
# - Parameters are fixed to small values (n=20, m=12, q=47, etc.) to keep the experiment feasible.
# - The script uses:
#     * Sage‚Äôs finite field + linear algebra (GF, random_matrix, rref, vector, matrix)
#     * SJT permutation ordering to update list values incrementally after adjacent swaps.
#
# ============================================================================
import random
import math
import numpy as np
import itertools
from collections import defaultdict
from functools import lru_cache
from sage.combinat.SJT import SJT


# ----------------------------------------------------------------------------
# Basic combinatorial primitive
# ----------------------------------------------------------------------------

@lru_cache(None)
def combination(n, k):
    """
    Binomial coefficient C(n, k) with caching.

    We cache to avoid recomputing combinatorial values repeatedly inside loops.
    """
    return int(math.comb(n, k))


# ----------------------------------------------------------------------------
# Steinhaus‚ÄìJohnson‚ÄìTrotter swap sequence
# ----------------------------------------------------------------------------

def compute_sjt_swap_sequence(n):
    """
    Precompute adjacent swap positions generated by the SJT order.

    The SJT algorithm enumerates permutations such that each next permutation
    differs from the previous by a single adjacent swap.

    Returns
    -------
    swap_positions : list[int]
        A list of length (n! - 1). Each entry is an index `idx` in {0,...,n-2}
        indicating that positions idx and idx+1 are swapped to get the next
        permutation in SJT order.

    ---------------------
    """
    swap_positions = []
    perm_state = SJT(list(range(1, n + 1)))

    for _ in range(math.factorial(n) - 1):
        previous = perm_state._list[:]
        perm_state = perm_state.next()
        current = perm_state._list[:]

        for idx in range(n - 1):
            if previous[idx] != current[idx]:
                swap_positions.append(idx)
                break

    return swap_positions




# ----------------------------------------------------------------------------
# Lexicographic subset rank. In paper it is denoted by ùúå.
# ----------------------------------------------------------------------------

@lru_cache(None)
def subset_rank(positions):
    """
    Combinatorial rank of a sorted subset of fixed size in lex order.

    Parameters
    ----------
    positions : tuple/list of ints
        Indices in {0,...,n-1}. The function sorts them internally.

    Returns
    -------
    total : int
        Rank in the lexicographic ordering of t-subsets.
    ---------
    """
    positions = sorted(list(positions))
    t = len(positions)

    total = 0
    prev = -1

    for j in range(ell):
        pos = positions[j]
        total += bin_lookup(n - prev - 1, t - j) - bin_lookup(n - pos, t - j)
        prev = pos

    return total


# ============================================================================
# Fixed experimental parameters
# ============================================================================
# n  : number of variables / coordinates
# n1 : number of rows in the random matrix A (before adding the all-ones row)
# m  : total number of equations after augmentation
# q  : field size

n = 20
n1 = 11
m = 12
q = 47

# Algorithm parameters
k = 4
ell = 6
r = n - m + k - ell

# ‚ÄúGuessed/marked‚Äù overlap sizes (Œº‚ÇÅ, Œº‚ÇÇ)
mu1 = 2
mu2 = 2


# ----------------------------------------------------------------------------
# Precompute SJT swap sequences
# ----------------------------------------------------------------------------

print("Precomputing SJT swap sequences...")
# swap_seq_l is the sequence of adjacent swaps for permutations of length ell
# swap_seq_r is the sequence of adjacent swaps for permutations of length r
swap_seq_l = compute_sjt_swap_sequence(ell)
swap_seq_r = compute_sjt_swap_sequence(r)

# ----------------------------------------------------------------------------
# Binomial lookup table
# ----------------------------------------------------------------------------

bin_table = matrix([[combination(i, j) for j in range(ell + 1)] for i in range(n + 1)])


def bin_lookup(a, b):
    if b < 0 or b > a:
        return 0
    return bin_table[a, b]



# ----------------------------------------------------------------------------
# Finite field setup
# ----------------------------------------------------------------------------

F = GF(q)
h = [q**i for i in range(k)]





# ----------------------------------------------------------------------------
# Compatibility table precomputation
# ----------------------------------------------------------------------------

print("Precomputing compatibility table...")

# compatibility_table[row_rank, col_rank] = True iff:
#   - row corresponds to an ‚Ñì-subset s1
#   - col corresponds to an r-subset s2
#   - s1 and s2 are disjoint (s2 chosen from indices not in s1)
#
# This accelerates the ‚Äúfiltering‚Äù step: after a collision, we check that
# the two supports do not overlap.

rows = combination(n, ell)
cols = combination(n, r)
compatibility_table = np.zeros((rows, cols), dtype=bool)

all_indices = list(range(n))

row_idx = 0
for s1 in itertools.combinations(all_indices, ell):
    remaining = list(set(all_indices) - set(s1))

    # For each row subset s1, iterate over r-subsets taken from the complement
    for s2 in itertools.combinations(remaining, r):
        col_idx = subset_rank(s2)
        compatibility_table[row_idx, col_idx] = True

    row_idx += 1

print("Precomputation done.")


# ============================================================================
# Experimental trials
# ============================================================================
# Statistics:
# - trials: number of random (LMD1, LMD2) samples attempted until one matches the planted split
# - collision_count: number of raw collisions encountered
# - filtered_collision_count: collisions that also pass the disjoint-support compatibility check

trials = 0
collision_count = 0
filtered_collision_count = 0


for iteration in range(1, 1001):

    # ------------------------------------------------------------------------
    # Generate random linear system and planted solution
    # ------------------------------------------------------------------------
    # given_A is a random n1 x n matrix over F.
    # Then we compute its rref and derive a planted solution V in a structured way:
    #   - sample the ‚Äúfree‚Äù part v2
    #   - solve for v1 so that the system constraints hold (using the rref structure)
    # Finally, we permute/shuffle coordinates (B = shuffled coordinates of V) to create
    # a PKP instance (given_A, B).

    given_A = random_matrix(F, n1, n)
    rref_A = given_A.rref()

    M_tmp = rref_A[:, n1:]

    flag = 1
    while flag < n:
        v2 = vector(F, random.sample(list(F), n - n1))
        v1 = -M_tmp * v2
        v = list(v1) + list(v2)
        V = vector(F, v)
        flag = len(set(list(V)))

    planted_solution = vector(F, V)
    B = list(V)

    random.shuffle(B)
    B_set = set(B)

    s1_star = V[m - k : m - k + ell]
    s2_star = V[m - k + ell :]


    # ------------------------------------------------------------------------
    # Augmented system reduction
    # ------------------------------------------------------------------------

    first_row = matrix(F, 1, n, [1] * n)
    A_tilde = first_row.stack(given_A)
    augmented = A_tilde.augment(A_tilde * V)
    rref_aug = augmented.rref()

    N = rref_aug[:, :n]
    t = vector(F, [rref_aug[i][n] for i in range(m)])

    M_block = N[ : m-k, m-k : ]
    t1 = t[: m - k]
    
    flag = 0
    while flag == 0:
        Q = random_matrix(F, k, k)
        flag = Q.det()

    M = Q * N[m-k :, m-k: ]
    t2 = Q * t[m - k :]

    M1 = M[:, :ell]
    M2 = M[:, ell:]


    # ------------------------------------------------------------------------
    # Random subset selection consistent with planted solution
    # ------------------------------------------------------------------------
    #
    # Choose two random sets of VALUES from B:
    #   LMD1 of size mu1 and LMD2 of size mu2
    # Repeat until those chosen values indeed lie in the planted split
    # (LMD1 ‚äÜ s1_star and LMD2 ‚äÜ s2_star).

    while True:
        trials += 1

        LMD1 = set(random.sample(list(B), mu1))
        remaining = list(B_set.difference(LMD1))
        LMD2 = set(random.sample(remaining, mu2))

        if LMD1.issubset(set(s1_star)) and LMD2.issubset(set(s2_star)):
            break

    # Convert chosen VALUES into their positions (indices) in the given list B.
    LMD1 = {B.index(a) for a in LMD1}
    LMD2 = {B.index(a) for a in LMD2}


    # ------------------------------------------------------------------------
    # Build list L1 using incremental SJT updates
    # ------------------------------------------------------------------------
    #
    # L1 maps a hash y to a list of candidates (perm, rank_val).
    # - `perm` is the current permutation (ordering) of the chosen support indices.
    # - `rank_val` is the rank of the support set (used to filter overlaps later).

    L1 = defaultdict(list)
    complement = set(all_indices) - (LMD1 | LMD2)

    # Enumerate subsets of size (ell - mu1) from complement,
    # then union with LMD1 to get an ‚Ñì-subset support.
    for s in Subsets(complement, ell - mu1):

        indices = sorted(list(s) + list(LMD1))

        #Compute the rank of the I-supp set
        rank_val = subset_rank(tuple(indices))

        perm = indices[:]
        s_vec = vector(F, [B[i] for i in perm])

        w_vec = M1 * s_vec
        y = sum(int(w_vec[i]) * h[i] for i in range(k))

        L1[y].append((perm[:], rank_val))

        # Precompute incremental swap vectors:
        # For a swap at position j (swapping entries u and v in perm),
        # the change in w_vec is:
        #    Œî = (u_value - v_value) * (M1.col(j) - M1.col(j+1)).
        #
        # Here keys are (u_value, v_value, j) storing that Œî.
        precomputed = {}
        for u_idx in indices:
            for v_idx in indices:
                for j in range(ell - 1):
                    diff = M1.column(j) - M1.column(j + 1)
                    precomputed[(B[u_idx], B[v_idx], j)] = (B[u_idx] - B[v_idx]) * diff

        # Apply SJT swaps
        # Each swap updates perm and w_vec incrementally (instead of recomputing M1*s_vec).
        for swap_pos in swap_seq_l:
            a, b = perm[swap_pos], perm[swap_pos + 1]
            perm[swap_pos], perm[swap_pos + 1] = b, a

            w_vec += precomputed[(B[b], B[a], swap_pos)]
            y = sum(int(w_vec[i]) * h[i] for i in range(k))

            L1[y].append((perm[:], rank_val))


    # ------------------------------------------------------------------------
    # Search for collisions using list L2
    # ------------------------------------------------------------------------
    #
    # For each candidate in L2:
    #   - compute y = encode(t2 - M2*s_vec)
    #   - look up collisions in L1[y]
    #   - then filter by compatibility (Compatibility Table)
    #   - finally verify the reconstructed solution satisfies constraints.

    solutions = []

    for s in Subsets(complement, r - mu2):

        indices = sorted(list(s) + list(LMD2))

        #Compute the rank of the I-supp set
        rank_val = subset_rank(tuple(indices))

        perm = indices[:]
        s_vec = vector(F, [B[i] for i in perm])

        w_vec = t2 - M2 * s_vec
        y = sum(int(w_vec[i]) * h[i] for i in range(k))

        # If collision exists, attempt to combine with L1 entries and filter/verify
        if L1[y]:
            collision_count += len(L1[y])

            for idx_s1, rank_s1 in L1[y]:
                if compatibility_table[rank_s1, rank_val]:
                    filtered_collision_count += 1

                    s1 = [B[i] for i in idx_s1]
                    s2 = [B[i] for i in perm]

                    x2 = vector(F, s1 + s2)
                    x1 = t1 - M_block * x2

                    # Check that x1 uses allowed values and is disjoint from x2
                    if set(x1) <= B_set and not set(x1) & set(x2):
                        solutions.append(vector(F, list(x1) + list(x2)))

        
        # Precompute incremental swap vectors
        precomputed = {}
        for u_idx in indices:
            for v_idx in indices:
                for j in range(ell - 1):
                    diff = M2.column(j) - M2.column(j + 1)
                    precomputed[(B[u_idx], B[v_idx], j)] = (B[u_idx] - B[v_idx]) * diff

        
                    
        # SJT enumeration of permutations
        # Update w_vec incrementally under each adjacent swap
        for swap_pos in swap_seq_r:
            a, b = perm[swap_pos], perm[swap_pos + 1]
            perm[swap_pos], perm[swap_pos + 1] = b, a

            w_vec -= precomputed[(B[b], B[a], swap_pos)]
            y = sum(int(w_vec[i]) * h[i] for i in range(k))

            if L1[y]:
                collision_count += len(L1[y])

                for idx_s1, rank_s1 in L1[y]:
                    if compatibility_table[rank_s1, rank_val]:
                        filtered_collision_count += 1

                        s1 = [B[i] for i in idx_s1]
                        s2 = [B[i] for i in perm]

                        x2 = vector(F, s1 + s2)
                        x1 = t1 - M_block * x2

                        if set(x1) <= B_set and not set(x1) & set(x2):
                            solutions.append(vector(F, list(x1) + list(x2)))

    # ------------------------------------------------------------------------
    # Verification and statistics
    # ------------------------------------------------------------------------
    #
    # - Confirm the planted solution is among found solutions.
    # - Warn if multiple solutions appear.
    # - Print running averages of:
    # - Print running averages of:
    #     (i) expected number of repetition of finding disjoint subsets that 
    #         lie in the planted split,
    #     (ii) collisions per iteration,
    #     (iii) filtered collisions per iteration.

    if planted_solution not in solutions:
        print("Solution not found:", planted_solution, solutions)

    if len(solutions) > 1:
        print("Multiple solutions:", len(solutions))

    print(
        iteration,
        (
            round(trials / iteration, 3),
            round(collision_count / iteration, 3),
            round(filtered_collision_count / iteration, 3),
        ),
    )
